{
    "docs": [
        {
            "location": "/", 
            "text": "Introduction\n\n\n\nArtificial neural networks (ANN) are computing architectures with massively parallel interconnections of simple neurons. It has been applied to biomedical fields such as imaging analysis and diagnosis.  \n\n\nIn this package built on the Theano math library, we implement ANNs to predict patient prognosis by extending Cox Regression to the non-linear neural network framework.  \n\n\nSome features of Cox-nnet include parallelization and GPU usage for high computational efficiency, training optimization methods such as the Nesterov accelerated gradient and flexibility in allowing the specification of neural network architecture.  Incorporation of biological information into the structure or using a deep learning approach are possible.  \n\n\nIn all, we think that this new tool will tremendously help researchers to build stronger predictive models as well as investigate the importance of variables associated with patient outcome.  \n\n\n Functions overview\n\n\n\n\n\ntrainCoxMlp\n - main function for training a Cox-nnet model\n\n\nCVLoglikelihood\n - calculate the cross-validated logliklihood (model performance metric)\n\n\nCIndex\n - calculate the C-Index (model performance metric)\n\n\nL2CVSearch\n - helper function for optimizing the regularization parameter; searches for lambda using a hill climbing algorithm\n\n\nL2CVProfile\n - helper function for optimizing the regularization parameter; profiles lambda across a range of values\n\n\nevalNewData\n - evaluates new data or test data; outputs the linear predictor (i.e., the log hazard ratio)\n\n\nvarImportance\n - determine variable importance through a dropout procedure\n\n\nsaveModel\n - saves model to binary file\n\n\nloadModel\n - loads model from file", 
            "title": "Home"
        }, 
        {
            "location": "/trainCoxMlp/", 
            "text": "trainCoxMlp\n\n\n\nThis function is the main entry point to building a Cox-nnet survival model.  Returns the Cox-nnet model and the likelihood score.  \n\n\nParameters:\n\n\n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nytime_train\n - time of death or censoring for each patient\n\n\nystatus_train\n - censoring of each patient\n\n\nmodel_params = {}\n - model parameters, see below\n\n\nsearch_params = {}\n - model search/training parameters, see below\n\n\nverbose=True\n - print more stuff.  \n\n\n\n\nmodel_params\n is a dictionary of model parameters used in model training.  It has the following parameters:\n\n\n\n\nL2_reg\n - Ridge regression parameter for regularization.  Default value is \nnumpy.exp(-1)\n\n\nnode_map\n - mapping of neurons in network\n\n\ninput_split\n - splitting of input layer for customization of the network architecture\n\n\n\n\nThe network architecture is defined by the \nnode_map\n and \ninput_split\n parameters.  Setting these values to \nNone\n will default to a neural network of one hidden layer with the number of neurons in the hidden layer = \nceil( sqrt( # of input nodes ))\n.  \ninput_split\n is a list of lists with the indices of the inputs determining which sub-input layer each input belongs to.  For example, if you want to split an input with four features into two groups of two: \ninput_split = [[0,1], [2,3]]\n.  \n\n\nnode_map\n defines the connections of the input layer (or sub-input layers).  \nnode_map\n list of list of triples (one list for each hidden layer).  A triple defines the inputs of a sub-hidden layer and how many output nodes it has.  I.e., \n( # of input neurons, list of input sub-layers, # of output neurons)\n.  The \nsimpleNetArch\n function can be used to generate an architecture with a given number of hidden layer neurons.\n\n\nsearch params\n is a dictionary of search/optimization hyper-parameters.  Generally the default values will work pretty well and you don't need to change these parameters.  \n\n\n\n\nmethod\n - The algorithm for gradient descent.  Includes standard gradient descent (\"gradient\"), Nesterov accelerated gradient \"nesterov\" and momentum gradient descent (\"momentum\").  Default is \"nesterov\".  \n\n\nlearning_rate\n - Initial learning rate.  Default is 0.01\n\n\nmomentum\n - proportion of momentum in momentum and nesterov gradients.  Default is 0.9.  \n\n\nlr_decay\n - Decrease of the learning rate if the cost function is not decreasing.  Default is 0.9\n\n\nlr_growth\n - Increase of the learning rate if the cost function is decreasing.  Default is 1.0 (i.e., it does not increase.  Adding a small term could, e.g., 1.01, could improve speed).  \n\n\neval_step\n - Number iterations between cost function evaluation in order to determine learning rate decay or growth.  Setting this to a lower number will increase overhead.  Default is 23.  \n\n\nmax_iter\n - Maximum number of iterations.  Default is 10000 \n\n\nstop_threshold\n - Threshold for stopping.  If the cost does not decrease by this proportion, then allow the training to stop.  Default is 0.995.\n\n\npatience\n - Perform at least many iterations before stopping. Default is 2000. \n\n\npatience_incr\n - If a new lowest cost is found, wait at least \npatience_incr * current iteration\n before stopping.  Default is 2.  \n\n\nrand_seed\n - Random seed for initializing model parameters.  Default is 123.", 
            "title": "trainCoxMlp"
        }, 
        {
            "location": "/simpleNetArch/", 
            "text": "simpleNetArch\n\n\n\nThis function is a helper function for generating the network architecture parameter.  A list object used as input for the \nnode_map\n parameter is returned.  \n\n\nParameters:\n\n\n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nn_nodes\n - number of nodes desired in the hidden layer.", 
            "title": "simpleNetArch"
        }, 
        {
            "location": "/CVLoglikelihood/", 
            "text": "CVLoglikelihood\n\n\n\nThis function calculates the cross validated log likelihood score of a validation set.  The score is calculated in the context of the training data by means of subtraction.  See \nHouwelingen et al., 2006\n for more information.  \n\n\nParameters:\n\n\n\n\nmodel\n - the model returned by \ntrainCoxMlp\n\n\nx_full\n - `2D numpy array of the training and validation set\n\n\nytime_full\n - time of death or censoring for each patient in both training and validation set\n\n\nystatus_full\n - censoring of each patient in both training and validation set\n\n\nx_train\n - 2D numpy array of the training set\n\n\nytime_train\n - time of death or censoring for each patient in the training set\n\n\nystatus_train\n - censoring of each patient in the training set\n\n\n\n\nReference: \nvan Houwelingen, Hans C., et al. \"Cross-validated Cox regression on microarray gene expression data.\" Statistics in medicine 25.18 (2006): 3201-3216.", 
            "title": "CVLoglikelihood"
        }, 
        {
            "location": "/CIndex/", 
            "text": "CIndex\n\n\n\nThis function calculates the Concordance Index (C-Index) of the model.  This statistic is the area under the curve in a classification model.  \n\n\nParameters:\n\n\n\n\nmodel\n - the model returned by \ntrainCoxMlp\n\n\nx_test\n - `2D numpy array of features\n\n\nytime_test\n - time of death or censoring for each patient\n\n\nystatus_test\n - censoring of each patient\n\n\n\n\nReference: \nHeagerty, Patrick J., and Yingye Zheng. \"Survival model predictive accuracy and ROC curves.\" Biometrics 61.1 (2005): 92-105.", 
            "title": "CIndex"
        }, 
        {
            "location": "/L2Profile/", 
            "text": "L2CVProfile\n\n\n\nThis is a function for evaluating model performance on a validation dataset for a range of L2 parameters.  It returns: likelihoods, a list of the score (log likelihood or C-index) for each fold; L2_reg_params, the log L2 parameters tested and mean_cvpl, the mean log likelihood for each L2 parameter tested.  \n\n\nParameters:\n\n\n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nytime_train\n - time of death or censoring for each patient\n\n\nystatus_train\n - censoring of each patient\n\n\nx_train\n - validation set matrix.  Expected numpy array.  \n\n\nytime_train\n - time of death or censoring for each patient on validation set\n\n\nystatus_train\n - censoring of each patient on validation set\n\n\nmodel_params = {}\n - model parameters, see \ntrainCoxMlp\n\n\nsearch_params = {}\n - model search/training parameters, see \ntrainCoxMlp\n\n\ncv_params\n - validation parameters, see below.  \n\n\nverbose=True\n - print more stuff.  \n\n\n\n\ncv_params\n is a dictionary of parameters for validation.  It has the following parameters:\n\n\n\n\ncv_metric\n - Performance metric for evaluating validation set performance.  \nloglikelihood\n or \ncindex\n.  Default is \nloglikelihood\n.   \n\n\nL2_range\n - List of(log) L2 parameters to cross-validate.", 
            "title": "L2Profile"
        }, 
        {
            "location": "/L2CVProfile/", 
            "text": "L2CVProfile\n\n\n\nThis is a function for performing cross validation on a list of given (log) values.  It returns: cv_likelihoods, a matrix of the cross validated log likelihoods for each fold; L2_reg_params, the log L2 parameters tested and mean_cvpl, the mean log likelihood for each L2 parameter tested.  \n\n\nParameters:\n\n\n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nytime_train\n - time of death or censoring for each patient\n\n\nystatus_train\n - censoring of each patient\n\n\nmodel_params = {}\n - model parameters, see \ntrainCoxMlp\n\n\nsearch_params = {}\n - model search/training parameters, see \ntrainCoxMlp\n\n\ncv_params\n - cross validation parameters, see below.  \n\n\nverbose=True\n - print more stuff.  \n\n\n\n\ncv_params\n is a dictionary of parameters for cross validation.  It has the following parameters:\n\n\n\n\ncv_seed\n - Random seed for splitting validation folds.  Default is 1.  \n\n\nn_folds\n - Number of folds to cross-validate.  Default is 10.   \n\n\ncv_metric\n - Performance metric for evaluating cross-validation performance.  \nloglikelihood\n or \ncindex\n.  Default is \nloglikelihood\n.  \n\n\nsearch_iters\n - Number of iterations in hill climbing.  Default is 3.  \n\n\nL2_range\n - List of(log) L2 parameters to cross-validate.  Default \n[-5,-4,-3,-2,-1]\n.", 
            "title": "L2CVProfile"
        }, 
        {
            "location": "/L2CVSearch/", 
            "text": "L2CVSearch\n\n\n\nThis is a function for performing cross validation for optimizing the L2 regularization parameter.  It uses a basic hill climbing algorithm to search a range of parameters.  It returns: cv_likelihoods, a matrix of the cross validated log likelihoods for each fold; L2_reg_params, the log L2 parameters tested and mean_cvpl, the mean log likelihood for each L2 parameter tested.  \n\n\nParameters:\n\n\n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nytime_train\n - time of death or censoring for each patient\n\n\nystatus_train\n - censoring of each patient\n\n\nmodel_params = {}\n - model parameters, see \ntrainCoxMlp\n\n\nsearch_params = {}\n - model search/training parameters, see \ntrainCoxMlp\n\n\ncv_params\n - cross validation parameters, see below.  \n\n\nverbose=True\n - print more stuff.  \n\n\n\n\ncv_params\n is a dictionary of parameters for cross validation.  It has the following parameters:\n\n\n\n\ncv_seed\n - Random seed for splitting validation folds.  Default is 1.  \n\n\nn_folds\n - Number of folds to cross-validate.  Default is 10.   \n\n\ncv_metric\n - Performance metric for evaluating cross-validation performance.  \nloglikelihood\n or \ncindex\n.  Default is \nloglikelihood\n.  \n\n\nsearch_iters\n - Number of iterations in hill climbing.  Default is 3.  \n\n\nL2_range\n - Range to search for L2 parameter.  Default \n[-5,-1]\n.", 
            "title": "L2CVSearch"
        }, 
        {
            "location": "/predictNewData/", 
            "text": "trainCoxMlp\n\n\n\nThis function makes prediction on new data.  The output is the linear predictor (log hazard ratio).  \n\n\nParameters:\n\n\n\n\nmodel\n - model produced by \ntrainCoxMlp\n\n\nx_test\n - Numpy array of features.", 
            "title": "predictNewData"
        }, 
        {
            "location": "/varImportance/", 
            "text": "varImportance\n\n\n\nThis function calculates relative variable importance by drop-out.  The values of a variable are set to its mean and the log likelihood is recalculated.  The difference between the original log likelihood and the new log likelihood is the variable importance.  \n\n\nParameters:\n\n\n\n\nmodel\n - the model returned by \ntrainCoxMlp\n \n\n\nx_train\n - training set matrix.  Expected numpy array.  \n\n\nytime_train\n - time of death or censoring for each patient\n\n\nystatus_train\n - censoring of each patient", 
            "title": "varImportance"
        }, 
        {
            "location": "/saveModel/", 
            "text": "saveModel\n\n\n\nA helper function for saving a model to a binary file.\n\n\nParameters:\n\n\n\n\nmodel\n - the model returned by \ntrainCoxMlp\n.\n\n\nfile\n - name of file to save to.", 
            "title": "saveModel"
        }, 
        {
            "location": "/loadModel/", 
            "text": "saveModel\n\n\n\nA helper function for loading a model from a binary file.\n\n\nParameters:\n\n\n\n\nfile\n - name of file to load from.", 
            "title": "loadModel"
        }, 
        {
            "location": "/examples/", 
            "text": "Example usages\n\n\n\nThis page contains two examples: \n\n\n\n\na primary biliary cirrhosis (PBC) dataset, containing 7 clinical variables (age, sex, edema, bilirubin concentration, albumin concentration, prothrombin time, and disease stage) with 410 patients\n\n\nkidney renal cell carcinoma (KIRC) dataset using RNASeq gene expression data.  Counts were normalized with DESeq2 and then log-ed (log2 + 1).  \n\n\n\n\nThe PBC dataset was obtained from the \"survival\" package in R and the KIRC dataset was obtained from The Cancer Genome Atlas via the Broad Institute.  Both datasets are standardized (mean = 0 and std. dev = 1) and located in the \nexamples\n folder.  \n\n\nPlots were done in R rather than Python due to better support for plotting survival data.  \n\n\nPBC example\n\n\nIn this example, the L2 parameter is optimized using the \nL2CVProfile\n function, which cross validates the training data.  A model is built using the \ntrainCoxMlp\n function, and evalulated on the hold out test data.  \n\n\nfrom cox_nnet import *\nimport numpy\nimport sklearn\n\n# load data\nx = numpy.loadtxt(fname=\nPBC/x.csv\n,delimiter=\n,\n,skiprows=0)\nytime = numpy.loadtxt(fname=\nPBC/ytime.csv\n,delimiter=\n,\n,skiprows=0)\nystatus = numpy.loadtxt(fname=\nPBC/ystatus.csv\n,delimiter=\n,\n,skiprows=0)\n\n# split into test/train sets\nx_train, x_test, ytime_train, ytime_test, ystatus_train, ystatus_test = \\\n    sklearn.cross_validation.train_test_split(x, ytime, ystatus, \n    test_size = 0.2, random_state = 100)\n\n#Define parameters\nmodel_params = dict(node_map = None, input_split = None)\nsearch_params = dict(method = \nnesterov\n, learning_rate=0.01, momentum=0.9,\n    max_iter=2000, stop_threshold=0.995, patience=1000, patience_incr=2, rand_seed = 123,\n    eval_step=23, lr_decay = 0.9, lr_growth = 1.0)\ncv_params = dict(cv_seed=1, n_folds=5, cv_metric = \nloglikelihood\n,\n    L2_range = numpy.arange(-4.5,1,0.5))\n\n#cross validate training set to determine lambda parameters\ncv_likelihoods, L2_reg_params, mean_cvpl = L2CVProfile(x_train,ytime_train,ystatus_train,\n    model_params,search_params,cv_params, verbose=False)\n\n\n\n\nIf you plot the data, you'll hopefully find a nice maximum to choose as the L2 parameter:\n\n\n\n\n#Build the model\nL2_reg = L2_reg_params[numpy.argmax(mean_cvpl)]\nmodel_params = dict(node_map = None, input_split = None, L2_reg=numpy.exp(L2_reg))\nmodel, cost_iter = trainCoxMlp(x_train, ytime_train, ystatus_train, \n    model_params, search_params, verbose=True)\ntheta = model.predictNewData(x_test)\n\n\n\n\nOne common way to visualize a survival model is to split the test data by median log hazard ratio and plot the two curves:\n\n\n\n\n\nKIRC example\n\n\nIn this example, the L2 parameter is optimized by a validation set (a subset of the training data) since cross-validation is computationally expensive.  High throughput datasets should be run using multiple cores or the GPU.  See: http://deeplearning.net/software/theano/tutorial/using_gpu.html\n\n\nfrom cox_nnet import *\nimport numpy\nimport sklearn\n\n# load data\nx = numpy.loadtxt(fname=\nKIRC/log_counts.csv.gz\n,delimiter=\n,\n,skiprows=0)\nytime = numpy.loadtxt(fname=\nKIRC/ytime.csv\n,delimiter=\n,\n,skiprows=0)\nystatus = numpy.loadtxt(fname=\nKIRC/ystatus.csv\n,delimiter=\n,\n,skiprows=0)\n\n# split into test/train sets\nx_train, x_test, ytime_train, ytime_test, ystatus_train, ystatus_test = \\\n    sklearn.cross_validation.train_test_split(x, ytime, ystatus, test_size = 0.2, random_state = 1)\n\n# split training into optimization and validation sets\nx_opt, x_validation, ytime_opt, ytime_validation, ystatus_opt, ystatus_validation = \\\n    sklearn.cross_validation.train_test_split(x_train, ytime_train, ystatus_train,\n    test_size = 0.2, random_state = 1)\n\n# set parameters\nmodel_params = dict(node_map = None, input_split = None)\nsearch_params = dict(method = \nnesterov\n, learning_rate=0.01, momentum=0.9, \n    max_iter=2000, stop_threshold=0.995, patience=1000, patience_incr=2, \n    rand_seed = 123, eval_step=23, lr_decay = 0.9, lr_growth = 1.0)\ncv_params = dict(cv_metric = \nloglikelihood\n, L2_range = numpy.arange(-2,1,0.5))\n\n#profile log likelihood to determine lambda parameter\nlikelihoods, L2_reg_params = L2Profile(x_opt,ytime_opt,ystatus_opt,\n    x_validation,ytime_validation,ystatus_validation,\n    model_params, search_params, cv_params, verbose=False)\n\nnumpy.savetxt(\nKIRC_likelihoods.csv\n, likelihoods, delimiter=\n,\n)\n\n\n\n\n\n\n#build model based on optimal lambda parameter\nL2_reg = L2_reg_params[numpy.argmax(likelihoods)]\nmodel_params = dict(node_map = None, input_split = None, L2_reg=numpy.exp(L2_reg))\nmodel, cost_iter = trainCoxMlp(x_train, ytime_train, ystatus_train, \n    model_params, search_params, verbose=True)\ntheta = model.predictNewData(x_test)\n\n\n\n\nInterfacing and analysis with R\n\n\nThe preferred programming environment for many bioinformaticians is R.  There are two ways to use Cox-nnet from R.  1) Using the rPython R package to interface with Cox-nnet (see http://rpython.r-forge.r-project.org/) or 2) calling Python scripts as a system call and reading the results in from the command line.  The 2nd method is detailed below:\n\n\n# Set Theano parameters\nPARAMS = \nTHEANO_FLAGS=mode=FAST_RUN,device=gpu0,floatX=float32 %s\n\n\n# Call python script\nsystem(sprintf(PARAMS,\nexamples/test_KIRC.py\n))\n\n# Read in the results on the test set\nas.numeric(readLines(\nKIRC_theta.csv\n)) -\n pred\nas.numeric(readLines(\nKIRC_ytime_test.csv\n)) -\n time\nas.numeric(readLines(\nKIRC_ystatus_test.csv\n)) -\n status\n\n# Plot survival curves\nlibrary(survival)\nsplit_r = as.numeric(pred \n median(pred))\nhigh_curve\n-survfit(formula = Surv(time[split_r == 1],status[split_r == 1]) ~ 1)  \nlow_curve\n-survfit(formula = Surv(time[split_r == 0],status[split_r == 0]) ~ 1) \nplot(high_curve,main=\n, xlab=\nTime to event (days)\n, ylab=\nProbability\n, col= \nblue\n, conf.int=F, lwd=1, xlim=c(0, max(time)))\nlines(low_curve, col= \ngreen\n, lwd=1, lty=1, conf.int=F)\nlegend(\ntopright\n, legend=c(\nHigh PI\n, \nLow PI\n), fill=c(\nblue\n,\ngreen\n))", 
            "title": "Examples"
        }, 
        {
            "location": "/examples/#pbc-example", 
            "text": "In this example, the L2 parameter is optimized using the  L2CVProfile  function, which cross validates the training data.  A model is built using the  trainCoxMlp  function, and evalulated on the hold out test data.    from cox_nnet import *\nimport numpy\nimport sklearn\n\n# load data\nx = numpy.loadtxt(fname= PBC/x.csv ,delimiter= , ,skiprows=0)\nytime = numpy.loadtxt(fname= PBC/ytime.csv ,delimiter= , ,skiprows=0)\nystatus = numpy.loadtxt(fname= PBC/ystatus.csv ,delimiter= , ,skiprows=0)\n\n# split into test/train sets\nx_train, x_test, ytime_train, ytime_test, ystatus_train, ystatus_test = \\\n    sklearn.cross_validation.train_test_split(x, ytime, ystatus, \n    test_size = 0.2, random_state = 100)\n\n#Define parameters\nmodel_params = dict(node_map = None, input_split = None)\nsearch_params = dict(method =  nesterov , learning_rate=0.01, momentum=0.9,\n    max_iter=2000, stop_threshold=0.995, patience=1000, patience_incr=2, rand_seed = 123,\n    eval_step=23, lr_decay = 0.9, lr_growth = 1.0)\ncv_params = dict(cv_seed=1, n_folds=5, cv_metric =  loglikelihood ,\n    L2_range = numpy.arange(-4.5,1,0.5))\n\n#cross validate training set to determine lambda parameters\ncv_likelihoods, L2_reg_params, mean_cvpl = L2CVProfile(x_train,ytime_train,ystatus_train,\n    model_params,search_params,cv_params, verbose=False)  If you plot the data, you'll hopefully find a nice maximum to choose as the L2 parameter:   #Build the model\nL2_reg = L2_reg_params[numpy.argmax(mean_cvpl)]\nmodel_params = dict(node_map = None, input_split = None, L2_reg=numpy.exp(L2_reg))\nmodel, cost_iter = trainCoxMlp(x_train, ytime_train, ystatus_train, \n    model_params, search_params, verbose=True)\ntheta = model.predictNewData(x_test)  One common way to visualize a survival model is to split the test data by median log hazard ratio and plot the two curves:", 
            "title": "PBC example"
        }, 
        {
            "location": "/examples/#kirc-example", 
            "text": "In this example, the L2 parameter is optimized by a validation set (a subset of the training data) since cross-validation is computationally expensive.  High throughput datasets should be run using multiple cores or the GPU.  See: http://deeplearning.net/software/theano/tutorial/using_gpu.html  from cox_nnet import *\nimport numpy\nimport sklearn\n\n# load data\nx = numpy.loadtxt(fname= KIRC/log_counts.csv.gz ,delimiter= , ,skiprows=0)\nytime = numpy.loadtxt(fname= KIRC/ytime.csv ,delimiter= , ,skiprows=0)\nystatus = numpy.loadtxt(fname= KIRC/ystatus.csv ,delimiter= , ,skiprows=0)\n\n# split into test/train sets\nx_train, x_test, ytime_train, ytime_test, ystatus_train, ystatus_test = \\\n    sklearn.cross_validation.train_test_split(x, ytime, ystatus, test_size = 0.2, random_state = 1)\n\n# split training into optimization and validation sets\nx_opt, x_validation, ytime_opt, ytime_validation, ystatus_opt, ystatus_validation = \\\n    sklearn.cross_validation.train_test_split(x_train, ytime_train, ystatus_train,\n    test_size = 0.2, random_state = 1)\n\n# set parameters\nmodel_params = dict(node_map = None, input_split = None)\nsearch_params = dict(method =  nesterov , learning_rate=0.01, momentum=0.9, \n    max_iter=2000, stop_threshold=0.995, patience=1000, patience_incr=2, \n    rand_seed = 123, eval_step=23, lr_decay = 0.9, lr_growth = 1.0)\ncv_params = dict(cv_metric =  loglikelihood , L2_range = numpy.arange(-2,1,0.5))\n\n#profile log likelihood to determine lambda parameter\nlikelihoods, L2_reg_params = L2Profile(x_opt,ytime_opt,ystatus_opt,\n    x_validation,ytime_validation,ystatus_validation,\n    model_params, search_params, cv_params, verbose=False)\n\nnumpy.savetxt( KIRC_likelihoods.csv , likelihoods, delimiter= , )   #build model based on optimal lambda parameter\nL2_reg = L2_reg_params[numpy.argmax(likelihoods)]\nmodel_params = dict(node_map = None, input_split = None, L2_reg=numpy.exp(L2_reg))\nmodel, cost_iter = trainCoxMlp(x_train, ytime_train, ystatus_train, \n    model_params, search_params, verbose=True)\ntheta = model.predictNewData(x_test)  Interfacing and analysis with R  The preferred programming environment for many bioinformaticians is R.  There are two ways to use Cox-nnet from R.  1) Using the rPython R package to interface with Cox-nnet (see http://rpython.r-forge.r-project.org/) or 2) calling Python scripts as a system call and reading the results in from the command line.  The 2nd method is detailed below:  # Set Theano parameters\nPARAMS =  THEANO_FLAGS=mode=FAST_RUN,device=gpu0,floatX=float32 %s \n\n# Call python script\nsystem(sprintf(PARAMS, examples/test_KIRC.py ))\n\n# Read in the results on the test set\nas.numeric(readLines( KIRC_theta.csv )) -  pred\nas.numeric(readLines( KIRC_ytime_test.csv )) -  time\nas.numeric(readLines( KIRC_ystatus_test.csv )) -  status\n\n# Plot survival curves\nlibrary(survival)\nsplit_r = as.numeric(pred   median(pred))\nhigh_curve -survfit(formula = Surv(time[split_r == 1],status[split_r == 1]) ~ 1)  \nlow_curve -survfit(formula = Surv(time[split_r == 0],status[split_r == 0]) ~ 1) \nplot(high_curve,main= , xlab= Time to event (days) , ylab= Probability , col=  blue , conf.int=F, lwd=1, xlim=c(0, max(time)))\nlines(low_curve, col=  green , lwd=1, lty=1, conf.int=F)\nlegend( topright , legend=c( High PI ,  Low PI ), fill=c( blue , green ))", 
            "title": "KIRC example"
        }
    ]
}